{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0d00092-2eca-485e-ad7c-a55438a356d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm_benchmark_bob import Network, predict_sequences, evaluate_predictions \n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27b6aa37-dfd2-4008-aa9f-9f1d50a35a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_data shape torch.Size([16000, 25, 5])\n",
      "tgt_data shape torch.Size([16000, 25])\n"
     ]
    }
   ],
   "source": [
    "from transformer_mpc import load_data_npz, normalize_batch, normalize_tensor, global_mean, global_std\n",
    "file_path = \"path_to_microgrid_data.npz\"\n",
    "## Generate Data\n",
    "\n",
    "\n",
    "cbuy_tensor, csell_tensor, cprod_tensor, power_res_tensor, power_load_tensor, x0_tensor, delta_transformed_tensor = load_data_npz(file_path)\n",
    "net_power_load = power_load_tensor - power_res_tensor\n",
    "src_data = torch.cat([\n",
    "    cbuy_tensor.unsqueeze(-1),  # Adding an extra dimension for feature alignment\n",
    "    csell_tensor.unsqueeze(-1),\n",
    "    cprod_tensor.unsqueeze(-1),\n",
    "    net_power_load.unsqueeze(-1),\n",
    "    x0_tensor.unsqueeze(1).repeat(1, cbuy_tensor.size(1), 1)  # Repeating x0 across the sequence length\n",
    "], dim=-1)  # Concatenate along the last dimension to combine features\n",
    "\n",
    "#Create target data \n",
    "tgt_data = delta_transformed_tensor.long()\n",
    "\n",
    "print(\"src_data shape\" , src_data.shape)\n",
    "print(\"tgt_data shape\" , tgt_data.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a97b292-d324-4775-bafd-54aa1aa7f050",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Training and Validation Dataset and Mini Batches\n",
    "mini_batch_size = 32 \n",
    "dataset_size = src_data.size(0)\n",
    "val_size = int(dataset_size * 0.2) \n",
    "train_size = dataset_size - val_size\n",
    "\n",
    "full_dataset = TensorDataset(src_data, tgt_data)\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=mini_batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=mini_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd72f499-8a3a-4420-aa46-fd8696de23f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n",
       "        23, 23, 23, 23, 23, 23, 23, 23, 23]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lstm_benchmark import Network, predict_sequences\n",
    "from transformer_mpc import compute_global_stats, normalize_tensor,  transform_predicted_to_binary_matrix, run_optimization, run_optimization_milp\n",
    "device = torch.device('cpu')\n",
    "\n",
    "model_lstm = Network(input_size=5, hidden_size=128, num_layers=1, lr=1e-4, n_actions=32)\n",
    "model_lstm.load_state_dict(torch.load('LSTM2_network_model.pth'))\n",
    "model_lstm.to(device)  # Ensure model is on the right device\n",
    "model_lstm.eval() \n",
    "src_batch, tgt_batch = next(iter(val_dataloader))  # Get a batch of data\n",
    "src_tensor_normalized = normalize_tensor(src_batch[0:1], global_mean, global_std)  # Normalize and use the first item as an example\n",
    "\n",
    "def predict_single_sequence(model, src_tensor_normalized, device):\n",
    "    \"\"\"\n",
    "    Predicts output for a single input sequence using the provided LSTM model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The trained LSTM model.\n",
    "    - src_tensor_normalized: A normalized input tensor.\n",
    "    - device: The device (CPU or GPU) the model is running on.\n",
    "    \n",
    "    Returns:\n",
    "    - predicted: The predicted output as a numpy array.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    src_tensor_normalized = src_tensor_normalized.to(device)  # Ensure the tensor is on the correct device\n",
    "    \n",
    "    # Initialize the hidden and cell states to zeros\n",
    "    h0 = torch.zeros(model.num_layers, src_tensor_normalized.size(0), model.hidden_size).to(device)\n",
    "    c0 = torch.zeros(model.num_layers, src_tensor_normalized.size(0), model.hidden_size).to(device)\n",
    "\n",
    "    with torch.no_grad():  # Context-manager that disables gradient calculation\n",
    "        outputs = model(src_tensor_normalized, h0, c0)\n",
    "        _, predicted = torch.max(outputs, 2)  # Get the class with the highest probability for each timestep\n",
    "\n",
    "    return predicted.cpu().numpy() \n",
    "    \n",
    "predict_single_sequence(model_lstm, src_tensor_normalized, device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e1183c4-1957-436e-b058-54d22a79db00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimality gap: 0.00%\n",
      "Average MILP time: 0.0040 seconds\n",
      "Average LP time: 0.0015 seconds\n",
      "Number of successful optimizations: 1/1\n"
     ]
    }
   ],
   "source": [
    "from transformer_mpc import gurobi_qp, hybrid_fhocp\n",
    "def run_optimization(delta, src_tensor, model_type):\n",
    "    try:\n",
    "        # Ensure input to gurobi_qp is correctly formatted\n",
    "        x0 = np.array(src_tensor[0,1,4])\n",
    "        net_power_load = src_tensor[:,:,3].numpy().squeeze()\n",
    "        cbuy = src_tensor[:,:,0].numpy().squeeze()\n",
    "        csell = src_tensor[:,:,1].numpy().squeeze()\n",
    "        cprod = src_tensor[:,:,2].numpy().squeeze()\n",
    "        \n",
    "        # Call gurobi_qp with validated inputs\n",
    "        mdl = gurobi_qp(x0, 25, net_power_load, cbuy, csell, cprod, delta)\n",
    "        \n",
    "\n",
    "        return mdl\n",
    "    except Exception as e:\n",
    "        print(f\"Exception during {model_type} model optimization: {e}\")\n",
    "        return None\n",
    "    \n",
    "def run_optimization_milp(src_tensor, model_type):\n",
    "    try:\n",
    "        # Ensure input to gurobi_qp is correctly formatted\n",
    "        x0 = np.array(src_tensor[0,1,4])\n",
    "        net_power_load = src_tensor[:,:,3].numpy().squeeze()\n",
    "        cbuy = src_tensor[:,:,0].numpy().squeeze()\n",
    "        csell = src_tensor[:,:,1].numpy().squeeze()\n",
    "        cprod = src_tensor[:,:,2].numpy().squeeze()\n",
    "        \n",
    "        # Call gurobi_qp with validated inputs\n",
    "        mdl_milp = hybrid_fhocp(x0, 25, net_power_load, cbuy, csell, cprod)\n",
    "\n",
    "\n",
    "        return mdl_milp\n",
    "    except Exception as e:\n",
    "        print(f\"Exception during {model_type} model optimization: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model_over_dataset(model, val_dataset, num_instances, device):\n",
    "    # Ensure num_instances does not exceed the length of the dataset\n",
    "    num_samples = min(num_instances, len(val_dataset))\n",
    "    \n",
    "    cost_actual_mem = []\n",
    "    cost_pred_mem = []\n",
    "    successful_optimizations = 0\n",
    "    milp_times = []\n",
    "    lp_times = []\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        # Randomly select an index\n",
    "        idx = np.random.randint(0, len(val_dataset))\n",
    "\n",
    "        # Fetch the data at the randomly selected index\n",
    "        src_tensor, tgt_tensor = val_dataset[idx]\n",
    "        src_tensor = src_tensor.unsqueeze(0)  # Add batch dimension if your model expects it\n",
    "        tgt_tensor = tgt_tensor.unsqueeze(0)  # Add batch dimension if your model expects it\n",
    "\n",
    "\n",
    "        # Normalize the source tensor\n",
    "        global_mean, global_std = compute_global_stats(val_dataset)  # Assume this function is defined correctly\n",
    "        src_tensor_normalized = normalize_tensor(src_tensor, global_mean, global_std)\n",
    "        # Actual model optimization using MILP\n",
    "        delta_actual = transform_predicted_to_binary_matrix(tgt_tensor)\n",
    "        \n",
    "        mdl_act = run_optimization(delta_actual, src_tensor, 'actual')\n",
    "        mdl_milp = run_optimization_milp(src_tensor, 'milp')\n",
    "        milp_times.append(mdl_milp.Runtime) \n",
    "\n",
    "        if mdl_act and mdl_act.status == gp.GRB.OPTIMAL:\n",
    "            cost_actual_mem.append(mdl_act.ObjVal)\n",
    "\n",
    "        # LSTM Model predictions and subsequent optimizations\n",
    "        start_time_pred = time.time()\n",
    "        predicted = predict_single_sequence(model, src_tensor, device)\n",
    "        predicted = torch.tensor(predicted)\n",
    "        prediction_time = time.time() - start_time_pred\n",
    "        # Convert predicted indices to actionable decisions if necessary\n",
    "        delta_predicted = transform_predicted_to_binary_matrix(predicted)  # This may need to be adjusted\n",
    "        mdl_pred = run_optimization(delta_predicted, src_tensor, model.__class__.__name__)\n",
    "        if mdl_pred and mdl_pred.status == gp.GRB.OPTIMAL:\n",
    "            successful_optimizations += 1\n",
    "            total_lp_time = prediction_time + mdl_pred.Runtime\n",
    "            lp_times.append(total_lp_time)\n",
    "            cost_pred_mem.append(mdl_pred.ObjVal)\n",
    "\n",
    "    # Calculate average costs and the optimality gap\n",
    "    avg_cost_actual = np.mean(cost_actual_mem) if cost_actual_mem else 0\n",
    "    avg_cost_pred = np.mean(cost_pred_mem) if cost_pred_mem else 0\n",
    "    optimality_gap = abs((avg_cost_pred - avg_cost_actual)) / avg_cost_actual if avg_cost_actual != 0 else 0\n",
    "    \n",
    "    avg_milp_time = np.mean(milp_times)\n",
    "    avg_lp_time = np.mean(lp_times)\n",
    "\n",
    "    # Output results\n",
    "    print(f\"Optimality gap: {optimality_gap:.2%}\")\n",
    "    print(f\"Average MILP time: {avg_milp_time:.4f} seconds\")\n",
    "    print(f\"Average LP time: {avg_lp_time:.4f} seconds\")\n",
    "    print(f\"Number of successful optimizations: {successful_optimizations}/{num_samples}\")\n",
    "\n",
    "\n",
    "num_instances = 1\n",
    "evaluate_model_over_dataset(model_lstm, val_dataset, num_instances, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8bb87be3-047c-4508-b17c-44010cbf23a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate with up to 1 model(s): 92.40%\n",
      "Success rate with up to 2 model(s): 96.40%\n",
      "Success rate with up to 3 model(s): 97.00%\n",
      "Success rate with up to 4 model(s): 99.40%\n",
      "Optimality gap TF: 0.76%\n",
      "Optimality gap LSTM: 0.18%\n",
      "Average MILP time: 0.0037 seconds\n",
      "Average LP time TF: 0.0487 seconds\n",
      "Average LP time LSTM: 0.0019 seconds\n",
      "Number of successful optimizations TF: 994/1000\n",
      "Number of successful optimizations LSTM: 999/1000\n"
     ]
    }
   ],
   "source": [
    "from transformer_mpc import try_model\n",
    "\n",
    "def evaluate_model_lstm_vs_transformer_over_dataset(model_lstm, tf_models, val_dataset, num_instances, device):\n",
    "    # Ensure num_instances does not exceed the length of the dataset\n",
    "    num_samples = min(num_instances, len(val_dataset))\n",
    "    \n",
    "    cost_actual_mem = []\n",
    "    cost_pred_lstm = []\n",
    "    cost_pred_tf = []\n",
    "    successful_optimizations_lstm = 0\n",
    "    successful_optimizations_tf = 0\n",
    "    milp_times = []\n",
    "    lp_times_lstm = []\n",
    "    lp_times_tf = []\n",
    "    stage_successes_tf = [0] * len(tf_models)\n",
    "\n",
    "\n",
    "    model_lstm.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        # Randomly select an index\n",
    "        idx = np.random.randint(0, len(val_dataset))\n",
    "\n",
    "        # Fetch the data at the randomly selected index\n",
    "        src_tensor, tgt_tensor = val_dataset[idx]\n",
    "        src_tensor = src_tensor.unsqueeze(0)  # Add batch dimension if your model expects it\n",
    "        tgt_tensor = tgt_tensor.unsqueeze(0)  # Add batch dimension if your model expects it\n",
    "\n",
    "\n",
    "        # Normalize the source tensor\n",
    "        global_mean, global_std = compute_global_stats(val_dataset)  # Assume this function is defined correctly\n",
    "        src_tensor_normalized = normalize_tensor(src_tensor, global_mean, global_std)\n",
    "        # Actual model optimization using MILP\n",
    "        delta_actual = transform_predicted_to_binary_matrix(tgt_tensor)\n",
    "        \n",
    "        mdl_act = run_optimization(delta_actual, src_tensor, 'actual')\n",
    "        mdl_milp = run_optimization_milp(src_tensor, 'milp')\n",
    "        milp_times.append(mdl_milp.Runtime) \n",
    "\n",
    "        if mdl_act and mdl_act.status == gp.GRB.OPTIMAL:\n",
    "            cost_actual_mem.append(mdl_act.ObjVal)\n",
    "\n",
    "        # LSTM Model predictions and subsequent optimizations\n",
    "        start_time_pred = time.time()\n",
    "        predicted = predict_single_sequence(model_lstm, src_tensor, device)\n",
    "        predicted = torch.tensor(predicted)\n",
    "        prediction_time = time.time() - start_time_pred\n",
    "        # Convert predicted indices to actionable decisions if necessary\n",
    "        delta_predicted_lstm = transform_predicted_to_binary_matrix(predicted)  # This may need to be adjusted\n",
    "        mdl_pred_lstm = run_optimization(delta_predicted_lstm, src_tensor, model.__class__.__name__)\n",
    "        if mdl_pred_lstm and mdl_pred_lstm.status == gp.GRB.OPTIMAL:\n",
    "            successful_optimizations_lstm += 1\n",
    "            total_lp_time = prediction_time + mdl_pred_lstm.Runtime\n",
    "            lp_times_lstm.append(total_lp_time)\n",
    "            cost_pred_lstm.append(mdl_pred_lstm.ObjVal)\n",
    "        \n",
    "        optimization_found = False\n",
    "        \n",
    "        for i, tf_model in enumerate(tf_models):\n",
    "            tf_model.eval()\n",
    "            start_time_pred = time.time() \n",
    "            delta_predicted_tf = try_model(tf_model, src_tensor_normalized)\n",
    "            prediction_time = time.time() - start_time_pred \n",
    "\n",
    "            mdl_pred_tf = run_optimization(delta_predicted_tf, src_tensor, 'Transformer')\n",
    "            if mdl_pred_tf and mdl_pred_tf.status == gp.GRB.OPTIMAL:\n",
    "                if not optimization_found:\n",
    "                    optimization_found = True\n",
    "                    total_lp_time = prediction_time + mdl_pred_tf.Runtime\n",
    "                    lp_times_tf.append(total_lp_time)\n",
    "                    cost_pred_tf.append(mdl_pred_tf.ObjVal)\n",
    "                    successful_optimizations_tf += 1\n",
    "                    for j in range(i, len(tf_models)):\n",
    "                        stage_successes_tf[j] += 1\n",
    "                    break\n",
    "                \n",
    "    # Calculate average costs and the optimality gap\n",
    "    avg_cost_actual = np.mean(cost_actual_mem) if cost_actual_mem else 0\n",
    "    avg_cost_tf = np.mean(cost_pred_tf) if cost_pred_tf else 0\n",
    "    avg_cost_lstm = np.mean(cost_pred_lstm) if cost_pred_lstm else 0\n",
    "\n",
    "    optimality_gap_tf = abs((avg_cost_tf - avg_cost_actual)) / avg_cost_actual if avg_cost_actual != 0 else 0\n",
    "    optimality_gap_lstm = abs((avg_cost_lstm - avg_cost_actual)) / avg_cost_actual if avg_cost_actual != 0 else 0\n",
    "\n",
    "    \n",
    "    \n",
    "    avg_milp_time = np.mean(milp_times)\n",
    "    avg_lp_time_tf = np.mean(lp_times_tf)\n",
    "    avg_lp_time_lstm = np.mean(lp_times_lstm)\n",
    "\n",
    "    for i in range(len(tf_models)):\n",
    "        success_rate_tf = (stage_successes_tf[i] / num_samples) * 100\n",
    "        print(f\"Success rate with up to {i+1} model(s): {success_rate_tf:.2f}%\")\n",
    "    \n",
    "\n",
    "    # Output results\n",
    "    print(f\"Optimality gap TF: {optimality_gap_tf:.2%}\")\n",
    "    print(f\"Optimality gap LSTM: {optimality_gap_lstm:.2%}\")\n",
    "    \n",
    "    print(f\"Average MILP time: {avg_milp_time:.4f} seconds\")\n",
    "    print(f\"Average LP time TF: {avg_lp_time_tf:.4f} seconds\")\n",
    "    print(f\"Average LP time LSTM: {avg_lp_time_lstm:.4f} seconds\")\n",
    "    \n",
    "    print(f\"Number of successful optimizations TF: {successful_optimizations_tf}/{num_samples}\")\n",
    "    print(f\"Number of successful optimizations LSTM: {successful_optimizations_lstm}/{num_samples}\")\n",
    "\n",
    "\n",
    "tf_models = [shallow, shallow_model2, shallow_model3, backup_model4]\n",
    "\n",
    "num_instances = 1000\n",
    "\n",
    "evaluate_model_lstm_vs_transformer_over_dataset(model_lstm, tf_models, val_dataset, num_instances, device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3bf4fea1-1f4b-4343-93d6-823126ad18c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate with up to 1 model(s): 93.20%\n",
      "Success rate with up to 2 model(s): 96.60%\n",
      "Success rate with up to 3 model(s): 97.10%\n",
      "Success rate with up to 4 model(s): 99.30%\n",
      "Optimality gap: 0.62%\n",
      "Average MILP time: 0.0038 seconds\n",
      "Average LP time: 0.0565 seconds\n"
     ]
    }
   ],
   "source": [
    "from transformer_mpc import cascaded_transformer\n",
    "\n",
    "\n",
    "evaluate_model_over_dataset_w_multiple_backups2(tf_models, val_dataset, num_instances)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
