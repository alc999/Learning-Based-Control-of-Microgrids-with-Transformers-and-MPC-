{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c487b14c-3ef5-4821-be4a-a1a23081bbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS available, current device MPS\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from transformer_onnx import save_model_and_config, load_model_and_config, train_transformer, validate_transformer, perform_inference\n",
    "from transformer_onnx import transform_predicted_to_binary_matrix, Transformer\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import time \n",
    "# from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import torch.onnx\n",
    "import onnxruntime as ort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b207712-98f5-471c-97a7-973cbef055c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_data shape torch.Size([16000, 25, 5])\n",
      "tgt_data shape torch.Size([16000, 25])\n"
     ]
    }
   ],
   "source": [
    "from Test_ONNX_backup import load_data_npz, normalize_batch, normalize_tensor\n",
    "\n",
    "file_path = \"path_to_microgrid_data\"\n",
    "\n",
    "cbuy_tensor, csell_tensor, cprod_tensor, power_res_tensor, power_load_tensor, x0_tensor, delta_transformed_tensor = load_data_npz(file_path)\n",
    "net_power_load = power_load_tensor - power_res_tensor\n",
    "src_data = torch.cat([\n",
    "    cbuy_tensor.unsqueeze(-1),  # Adding an extra dimension for feature alignment\n",
    "    csell_tensor.unsqueeze(-1),\n",
    "    cprod_tensor.unsqueeze(-1),\n",
    "    net_power_load.unsqueeze(-1),\n",
    "    x0_tensor.unsqueeze(1).repeat(1, cbuy_tensor.size(1), 1)  # Repeating x0 across the sequence length\n",
    "], dim=-1)  # Concatenate along the last dimension to combine features\n",
    "\n",
    "\n",
    "#Create target data \n",
    "tgt_data = delta_transformed_tensor.long()\n",
    "\n",
    "print(\"src_data shape\" , src_data.shape)\n",
    "print(\"tgt_data shape\" , tgt_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49f748e0-d352-45b3-b767-ba789905fa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch_size = 32 \n",
    "dataset_size = src_data.size(0)\n",
    "val_size = int(dataset_size * 0.2) \n",
    "train_size = dataset_size - val_size\n",
    "\n",
    "full_dataset = TensorDataset(src_data, tgt_data)\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=mini_batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=mini_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aef7feb-8b9a-49d8-95b6-439f06f7f949",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_onnx import evaluate_model_over_dataset_w_multiple_backups2\n",
    "transformer, config = load_model_and_config('trained_transformer2.pth')\n",
    "shallow, config_shallow  = load_model_and_config('shallow_transformer.pth')\n",
    "\n",
    "models =  ['name_of_transformer_models]\n",
    "num_instances = 1\n",
    "evaluate_model_over_dataset_w_multiple_backups2(models, val_dataset, num_instances)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439a6063-47d1-459a-92cd-10ad897c0ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_onnx import export_model_to_onnx, normalize_tensor, global_mean, global_std\n",
    "src_batch, tgt_batch = next(iter(val_dataloader))  # Get a batch of data\n",
    "src_tensor_normalized = normalize_tensor(src_batch[0:1], global_mean, global_std)  # Normalize and use the first item as an example\n",
    "\n",
    "# Use a realistic target tensor from your data\n",
    "real_tgt_tensor = tgt_batch[0:1]  # Use the first target sequence from the batch\n",
    "print(real_tgt_tensor.shape)\n",
    "export_model_to_onnx(shallow, src_tensor_normalized, real_tgt_tensor, \"shallow\")\n",
    "export_model_to_onnx(shallow_model2, src_tensor_normalized, real_tgt_tensor, \"shallow2\")\n",
    "export_model_to_onnx(shallow_model3, src_tensor_normalized, real_tgt_tensor, \"shallow3\")\n",
    "export_model_to_onnx(backup_model4, src_tensor_normalized, real_tgt_tensor, \"backup4\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c522f2aa-df9b-4988-941e-9566a6e7355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_onnx import run_onnx_inference, convert_logits_to_tokens\n",
    "\n",
    "src_batch, tgt_batch = next(iter(val_dataloader))  # Get a batch of data\n",
    "src_tensor_normalized = normalize_tensor(src_batch[0:1], global_mean, global_std)  # Normalize and use the first item as an example\n",
    "# Use a realistic target tensor from  data\n",
    "real_tgt_tensor = tgt_batch[0:1]  # Use the first target sequence from the batch\n",
    "logits = run_onnx_inference(\"shallow.onnx\", src_tensor_normalized, real_tgt_tensor)\n",
    "\n",
    "predicted_sequence_onnx = convert_logits_to_tokens(logits)\n",
    "print(\"onnx predicted sequence\",predicted_sequence_onnx)\n",
    "\n",
    "\n",
    "predicted_sequence = perform_inference(shallow, src_tensor_normalized)\n",
    "print(predicted_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b241d99-17c9-4957-9b40-314cc061bb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_onnx import cascaded_transformer\n",
    "\n",
    "models =  ['name_transformers']\n",
    "num_instances = 1\n",
    "cascaded_transformer(models, val_dataset, num_instances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea47042-ce80-4f7a-a9e6-1fcb19ceca69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_onnx import evaluate_models\n",
    "\n",
    "models_onnx = [\"name_exported_onnx_transformers\"]\n",
    "models = [shallow, shallow_model2, shallow_model3, backup_model4]\n",
    "num_instances = 1000\n",
    "evaluate_models(models, models_onnx, val_dataset, num_instances) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
