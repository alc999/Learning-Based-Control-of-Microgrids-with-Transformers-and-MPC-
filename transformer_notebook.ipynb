{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f62ae9d4-0f60-4074-8626-35493dd05ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS available, current device MPS\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from transformer_mpc import save_model_and_config, load_model_and_config, train_transformer, validate_transformer, perform_inference\n",
    "from testnormalise import transform_predicted_to_binary_matrix, Transformer\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43453c05-029d-407e-843e-50c720c0856a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_data shape torch.Size([16000, 48, 5])\n",
      "tgt_data shape torch.Size([16000, 48])\n"
     ]
    }
   ],
   "source": [
    "## Load and Generate Data \n",
    "from transformer_mpc import load_data_npz, normalize_batch\n",
    "#file_path = \"/Users/bobvanderwoude/Documents/Systems And Control/Afstuderen/Thesis/Transformer Model/Data_Microgrid/Data_Microgrid/Microgrid_aggregated_data_LARGE_20240307.npz\"\n",
    "## Generate Data\n",
    "file_path = \"path_to_microgrid_data\"\n",
    "\n",
    "cbuy_tensor, csell_tensor, cprod_tensor, power_res_tensor, power_load_tensor, x0_tensor, delta_transformed_tensor = load_data_npz(file_path)\n",
    "net_power_load = power_load_tensor - power_res_tensor\n",
    "src_data = torch.cat([\n",
    "    cbuy_tensor.unsqueeze(-1),  # Adding an extra dimension for feature alignment\n",
    "    csell_tensor.unsqueeze(-1),\n",
    "    cprod_tensor.unsqueeze(-1),\n",
    "    net_power_load.unsqueeze(-1),\n",
    "    x0_tensor.unsqueeze(1).repeat(1, cbuy_tensor.size(1), 1)  # Repeating x0 across the sequence length\n",
    "], dim=-1)  # Concatenate along the last dimension to combine features\n",
    "\n",
    "#Create target data \n",
    "tgt_data = delta_transformed_tensor.long()\n",
    "\n",
    "print(\"src_data shape\" , src_data.shape)\n",
    "print(\"tgt_data shape\" , tgt_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "869f69d7-67ac-4119-8d9a-6db2908af6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Training and Validation Dataset and Mini Batches\n",
    "mini_batch_size = 32 \n",
    "dataset_size = src_data.size(0)\n",
    "val_size = int(dataset_size * 0.2) \n",
    "train_size = dataset_size - val_size\n",
    "\n",
    "full_dataset = TensorDataset(src_data, tgt_data)\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=mini_batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=mini_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8d3bee-1342-424b-bcd4-092046d9eb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create transformer configuration\n",
    "\n",
    "config =  {'src_vocab_size': None, \n",
    "           'tgt_vocab_size': 32, \n",
    "           'd_model': 128,\n",
    "           'num_heads': 8,\n",
    "           'num_layers': 4,\n",
    "           'd_ff': 2048, \n",
    "           'feature_dim': 5, \n",
    "           'max_seq_length': 48, # src_data.size(1),\n",
    "           'dropout': 0.1, \n",
    "           'nr_epochs': 20, \n",
    "           'lr': 0.0001}\n",
    "transformer = Transformer(\n",
    "        src_vocab_size=config['src_vocab_size'], \n",
    "        tgt_vocab_size=config['tgt_vocab_size'], \n",
    "        d_model=config['d_model'], \n",
    "        num_heads=config['num_heads'], \n",
    "        num_layers=config['num_layers'], \n",
    "        d_ff=config['d_ff'], \n",
    "        feature_dim=config['feature_dim'], \n",
    "        max_seq_length=config['max_seq_length'], \n",
    "        dropout=config['dropout'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f169669d-23db-4b0e-a094-b47857e398e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train a Transformer \n",
    "train_transformer(transformer, train_dataloader, val_dataloader, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6402516a-aeac-40e0-adff-392778bb1041",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Trained Transformers and Configurations \n",
    "\n",
    "transformer, config = load_model_and_config('name_of_trained_transformer.pth')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"config = \", config)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecc3fe0-71da-4a04-b4e5-e809a4bd3e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cascaded_transformer\n",
    "from transformer_mpc import cascaded_transformer\n",
    "models = []  #transformer_models_for_cascaded_transformer \n",
    "num_instances = 1000\n",
    "\n",
    "\n",
    "cascaded_transformer(models4, val_dataset, num_instances)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08044aa-1030-4175-9200-15dbd5355b1b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
